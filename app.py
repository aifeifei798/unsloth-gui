"""
=====================================================================================
ğŸš€ Unsloth GUI Trainer: ä¸€ä¸ªä¸“ä¸šçš„äº¤äº’å¼å¾®è°ƒå·¥ä½œå° (v3.1 - æœ€ç»ˆç‰ˆ) ğŸš€
=====================================================================================
æ­¤ç‰ˆæœ¬æ–°å¢äº†æ ¸å¿ƒåŠŸèƒ½ï¼š
- åœ¨è®­ç»ƒæˆåŠŸç»“æŸåï¼Œè‡ªåŠ¨ä¿å­˜æœ€ç»ˆçš„ã€å¯ç”¨äºæ¨ç†çš„ LoRA é€‚é…å™¨ã€‚
"""

import gradio as gr
import torch
import os
import subprocess
import time
import json
from pathlib import Path

# æé«˜ PyTorch Dynamo çš„é‡ç¼–è¯‘å®¹å¿ä¸Šé™
torch._dynamo.config.recompile_limit = 100

from unsloth import FastLanguageModel
from transformers import TrainingArguments
from transformers.integrations import TensorBoardCallback
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets, load_from_disk

# --- (å‰é¢çš„ä»£ç ï¼Œå¦‚å…¨å±€é…ç½®å’Œè¾…åŠ©å‡½æ•°ï¼Œä¿æŒä¸å˜) ---
# --- 1. å…¨å±€æ ¸å¿ƒé…ç½® ---
LOGS_PARENT_DIR = "logs"
OUTPUTS_PARENT_DIR = "outputs"
TENSORBOARD_PROC = None


# --- 2. è¾…åŠ©å‡½æ•°ï¼šé…ç½®åŠ è½½ ---
def load_config_from_json_files(config_path, default_config, error_msg):
    if isinstance(config_path, Path) and config_path.is_dir():
        json_files = list(config_path.glob("*.json"))
        if not json_files:
            return [default_config], [default_config.get("display_name", "Default")]
        configs = [json.load(open(f, "r", encoding="utf-8")) for f in json_files]
        return configs, [cfg["display_name"] for cfg in configs]
    elif Path(config_path).is_file():
        with open(config_path, "r", encoding="utf-8") as f:
            configs = json.load(f)
        return configs, [cfg["display_name"] for cfg in configs]
    else:
        print(f"è­¦å‘Š: {error_msg}")
        return [default_config], [default_config.get("display_name", "Default")]


MODELS_CONFIG, MODEL_DISPLAY_NAMES = load_config_from_json_files(
    Path("models.json"), {}, "æ¨¡å‹é…ç½®æ–‡ä»¶ 'models.json' æœªæ‰¾åˆ°ã€‚"
)
DATASETS_CONFIG, DATASET_DISPLAY_NAMES = load_config_from_json_files(
    Path("datasets_config"), {}, "æ•°æ®é›†é…ç½®ç›®å½• 'datasets_config' æœªæ‰¾åˆ°ã€‚"
)


# --- 3. è¾…åŠ©å‡½æ•°ï¼šå¯åŠ¨ TensorBoard ---
def launch_tensorboard():
    global TENSORBOARD_PROC
    if TENSORBOARD_PROC is not None and TENSORBOARD_PROC.poll() is None:
        return
    os.makedirs(LOGS_PARENT_DIR, exist_ok=True)
    TENSORBOARD_PROC = subprocess.Popen(
        [
            "tensorboard",
            "--logdir",
            LOGS_PARENT_DIR,
            "--host",
            "0.0.0.0",
            "--port",
            "6006",
        ]
    )
    time.sleep(5)


# --- 4. è¾…åŠ©å‡½æ•°ï¼šå‡†å¤‡æ•°æ®é›† ---
def prepare_dataset(dataset_config):
    if dataset_config.get("is_local", False):
        dataset_path = dataset_config["dataset_id"]
        if not Path(dataset_path).exists():
            raise FileNotFoundError(f"æœ¬åœ°æ•°æ®é›†è·¯å¾„æœªæ‰¾åˆ°: {dataset_path}")
        dataset = load_from_disk(dataset_path)
    else:
        dataset = load_dataset(
            dataset_config["dataset_id"], split=dataset_config.get("split", "train")
        )

    prompt_template = dataset_config["prompt_template"]
    column_mappings = dataset_config["input_columns"]

    def formatting_prompts_func(examples):
        texts = []
        zipped_columns = zip(
            *(examples[col_name] for col_name in column_mappings.values())
        )
        for values in zipped_columns:
            format_dict = dict(zip(column_mappings.keys(), values))
            texts.append(prompt_template.format(**format_dict))
        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)

    if len(dataset) > 200 and not dataset_config.get("use_full_dataset", False):
        dataset = dataset.select(range(200))
    return dataset


# --- 5. æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•° (å·²æ›´æ–°) ---
def train_model(
    experiment_name,
    resume_training,
    training_mode,
    num_epochs,
    max_steps,
    save_steps,
    selected_model_name,
    selected_dataset_names,
    lora_r,
    lora_alpha,
    batch_size,
    grad_accum,
    optimizer,
    lr,
    progress=gr.Progress(track_tqdm=True),
):
    """æ¥æ”¶æ‰€æœ‰UIå‚æ•°å¹¶æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹å¾®è°ƒæµç¨‹ã€‚"""
    # ... (å‰é¢çš„ 1-4 æ­¥ï¼šè·¯å¾„è®¾ç½®ã€æ•°æ®åŠ è½½ã€æ¨¡å‹åˆå§‹åŒ–ã€LoRAé…ç½®ï¼Œä¿æŒä¸å˜) ...
    if not experiment_name or not experiment_name.strip():
        return "é”™è¯¯ï¼šå®éªŒåç§°ä¸èƒ½ä¸ºç©ºã€‚"
    experiment_name = experiment_name.strip().replace(" ", "_")
    output_dir = Path(OUTPUTS_PARENT_DIR) / experiment_name
    logging_dir = Path(LOGS_PARENT_DIR) / experiment_name
    progress(0, desc=f"å‡†å¤‡å®éªŒ: {experiment_name}")

    model_config = next(
        (item for item in MODELS_CONFIG if item["display_name"] == selected_model_name),
        None,
    )
    if not model_config or not selected_dataset_names:
        return "é”™è¯¯ï¼šå¿…é¡»é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œè‡³å°‘ä¸€ä¸ªæ•°æ®é›†ã€‚"
    progress(0.1, desc="åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")
    all_datasets = [
        prepare_dataset(cfg)
        for name in selected_dataset_names
        if (
            cfg := next(
                (item for item in DATASETS_CONFIG if item["display_name"] == name), None
            )
        )
    ]
    if not all_datasets:
        return "é”™è¯¯ï¼šæ— æ³•åŠ è½½æ‰€é€‰çš„æ•°æ®é›†é…ç½®ã€‚"
    progress(0.2, desc=f"åˆå¹¶ {len(all_datasets)} ä¸ªæ•°æ®é›†ä¸­...")
    combined_dataset = concatenate_datasets(all_datasets)

    progress(0.3, desc=f"åˆå§‹åŒ–æ¨¡å‹: {model_config['model_id']}...")
    dtype_str = model_config.get("dtype")
    dtype = {"bfloat16": torch.bfloat16, "float16": torch.float16}.get(dtype_str)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config["model_id"],
        max_seq_length=2048,
        dtype=dtype,
        load_in_4bit=model_config["load_in_4bit"],
    )
    model = FastLanguageModel.get_peft_model(
        model,
        r=int(lora_r),
        lora_alpha=int(lora_alpha),
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )

    # 5. æ ¹æ®é€‰æ‹©çš„æ¨¡å¼ï¼ŒåŠ¨æ€é…ç½®è®­ç»ƒå‚æ•°
    progress(0.4, desc="é…ç½®è®­ç»ƒå‚æ•°...")
    args = {
        "output_dir": str(output_dir),
        "logging_dir": str(logging_dir),
        "per_device_train_batch_size": int(batch_size),
        "gradient_accumulation_steps": int(grad_accum),
        "learning_rate": float(lr),
        "logging_steps": 1,
        "optim": optimizer,
        "fp16": not torch.cuda.is_bf16_supported(),
        "bf16": torch.cuda.is_bf16_supported(),
        "warmup_steps": 10,
        "weight_decay": 0.01,
        "lr_scheduler_type": "linear",
        "seed": 3407,
        "save_total_limit": 3,
    }

    if training_mode == "æŒ‰è½®æ¬¡ (Epochs)":
        args["num_train_epochs"] = float(num_epochs)
        args["save_strategy"] = "epoch"
    else:
        args["max_steps"] = int(max_steps)
        args["save_strategy"] = "steps"
        args["save_steps"] = int(save_steps)

    training_args = TrainingArguments(**args)

    # 6. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=combined_dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        dataset_num_proc=2,
        packing=False,
        args=training_args,
        callbacks=[TensorBoardCallback()],
    )

    # 7. å¼€å§‹æˆ–ç»§ç»­è®­ç»ƒ
    status_msg = "ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ..." if resume_training else "å¼€å§‹æ–°çš„è®­ç»ƒ..."
    progress(0.5, desc=status_msg)
    trainer.train(resume_from_checkpoint=resume_training)

    # vvvvvvvvvvvvvvvvvvvvvvvvvvvv  è¿™æ˜¯æ–°å¢çš„æ ¸å¿ƒåŠŸèƒ½ vvvvvvvvvvvvvvvvvvvvvvvvvvvv
    # 8. è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æœ€ç»ˆçš„ã€å¯ç”¨äºæ¨ç†çš„ LoRA é€‚é…å™¨
    progress(0.9, desc="è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆ LoRA é€‚é…å™¨...")

    # ä½¿ç”¨ save_pretrained ä¿å­˜ LoRA é€‚é…å™¨æƒé‡å’Œé…ç½®
    # è¿™ä¼šç”Ÿæˆ adapter_model.safetensors å’Œ adapter_config.json ç­‰æ–‡ä»¶
    model.save_pretrained(str(output_dir))

    # åŒæ—¶ä¿å­˜ tokenizerï¼Œè¿™å¯¹äºåç»­åŠ è½½å’Œä½¿ç”¨æ¨¡å‹éå¸¸é‡è¦
    tokenizer.save_pretrained(str(output_dir))

    progress(1.0, desc="æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    return f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆ LoRA é€‚é…å™¨å·²ä¿å­˜åœ¨ '{output_dir}'"
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# --- (6. Gradio UI å®šä¹‰ å’Œ 7. å¯åŠ¨åº”ç”¨ çš„ä»£ç ä¿æŒä¸å˜) ---
# --- 6. Gradio UI ç•Œé¢å®šä¹‰ ---
def update_training_mode_ui(mode):
    is_epoch_mode = mode == "æŒ‰è½®æ¬¡ (Epochs)"
    return {
        num_epochs_slider: gr.update(visible=is_epoch_mode),
        max_steps_slider: gr.update(visible=not is_epoch_mode),
        save_steps_input: gr.update(visible=not is_epoch_mode),
    }


with gr.Blocks(theme=gr.themes.Soft(), css="footer {display: none !important}") as demo:
    gr.Markdown("# ğŸš€ Unsloth + Gradio + TensorBoard (ç»ˆæç‰ˆ)")
    gr.Markdown("### âœ¨ *é»˜è®¤å‚æ•°å·²ä¸º 8GB VRAM (å¦‚ RTX 3070) ä¼˜åŒ–*")

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## âš™ï¸ è®­ç»ƒé…ç½®")
            with gr.Accordion("1. å®éªŒè®¾ç½®", open=True):
                experiment_name_input = gr.Textbox(
                    label="å®éªŒåç§° (å¿…å¡«)", value="8gb-vram-test"
                )
                resume_checkbox = gr.Checkbox(label="ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ", value=False)

            with gr.Accordion("2. æ¨¡å‹ä¸æ•°æ®é›†", open=True):
                model_dropdown = gr.Dropdown(
                    choices=MODEL_DISPLAY_NAMES,
                    value=MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None,
                    label="é€‰æ‹©æ¨¡å‹",
                )
                dataset_dropdown = gr.Dropdown(
                    choices=DATASET_DISPLAY_NAMES,
                    value=[DATASET_DISPLAY_NAMES[0]] if DATASET_DISPLAY_NAMES else None,
                    label="é€‰æ‹©æ•°æ®é›† (å¯å¤šé€‰)",
                    multiselect=True,
                )

            with gr.Accordion("3. LoRA å‚æ•° (8GB ä¼˜åŒ–)", open=False):
                lora_r_slider = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank (r)")
                lora_alpha_slider = gr.Slider(
                    4, 128, value=16, step=4, label="LoRA Alpha"
                )

            with gr.Accordion("4. è®­ç»ƒæ ¸å¿ƒå‚æ•° (8GB ä¼˜åŒ–)", open=True):
                training_mode_selector = gr.Radio(
                    ["æŒ‰æ­¥æ•° (Steps)", "æŒ‰è½®æ¬¡ (Epochs)"],
                    value="æŒ‰æ­¥æ•° (Steps)",
                    label="è®­ç»ƒæ¨¡å¼",
                )
                num_epochs_slider = gr.Slider(
                    0.1, 10, value=1, step=0.1, label="è®­ç»ƒè½®æ•° (Epochs)", visible=False
                )
                max_steps_slider = gr.Slider(
                    10,
                    2000,
                    100,
                    step=10,
                    label="æœ€å¤§è®­ç»ƒæ­¥æ•° (Max Steps)",
                    visible=True,
                )
                save_steps_input = gr.Number(
                    value=50, label="æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡æ–­ç‚¹", visible=True
                )

                batch_size_slider = gr.Slider(
                    1, 16, value=1, step=1, label="Batch Size"
                )
                grad_accum_slider = gr.Slider(
                    1, 16, value=8, step=1, label="Gradient Accumulation"
                )
                optimizer_dropdown = gr.Dropdown(
                    ["adamw_8bit", "adamw_torch"], value="adamw_8bit", label="ä¼˜åŒ–å™¨"
                )
                learning_rate_slider = gr.Slider(
                    1e-5, 5e-4, 2e-4, step=1e-5, label="å­¦ä¹ ç‡"
                )

            start_button = gr.Button("âœ… å¼€å§‹è®­ç»ƒ", variant="primary")

        with gr.Column(scale=3):
            gr.Markdown("## ğŸ“Š TensorBoard ç›‘æ§é¢æ¿")
            tensorboard_html = f'<iframe src="http://127.0.0.1:6006" width="100%" height="800px" frameborder="0"></iframe>'
            tensorboard_view = gr.HTML(tensorboard_html)

    gr.Markdown("--- \n ## ğŸ“‹ è®­ç»ƒæ—¥å¿—ä¸çŠ¶æ€")
    status_output = gr.Textbox(label="Status", interactive=False, lines=3, max_lines=5)

    training_mode_selector.change(
        fn=update_training_mode_ui,
        inputs=training_mode_selector,
        outputs=[num_epochs_slider, max_steps_slider, save_steps_input],
    )

    all_inputs = [
        experiment_name_input,
        resume_checkbox,
        training_mode_selector,
        num_epochs_slider,
        max_steps_slider,
        save_steps_input,
        model_dropdown,
        dataset_dropdown,
        lora_r_slider,
        lora_alpha_slider,
        batch_size_slider,
        grad_accum_slider,
        optimizer_dropdown,
        learning_rate_slider,
    ]
    start_button.click(fn=train_model, inputs=all_inputs, outputs=[status_output])

    demo.load(fn=launch_tensorboard)

# --- 7. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    demo.launch(share=True)
