"""
=====================================================================================
ğŸš€ Unsloth GUI Trainer & Playground (v3.8 - æ•°æ®æˆªæ–­æ§åˆ¶) ğŸš€
=====================================================================================
æ­¤ç‰ˆæœ¬æ ¹æ®ç”¨æˆ·åé¦ˆï¼Œæ–°å¢äº†æ ¸å¿ƒçš„æ˜“ç”¨æ€§åŠŸèƒ½ï¼š
1.  åœ¨UIç•Œé¢ä¸Šå¢åŠ äº†ä¸€ä¸ªå¤é€‰æ¡†ï¼Œå…è®¸ç”¨æˆ·è‡ªç”±é€‰æ‹©æ˜¯å¦æˆªæ–­æ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚
2.  é»˜è®¤å¼€å¯æˆªæ–­ï¼Œä»¥ä¿è¯æ–°æ‰‹çš„åˆæ¬¡ä½“éªŒæµç•…ã€å¿«é€Ÿã€‚
3.  ç§»é™¤äº†æ—§çš„ã€åŸºäºJSONé…ç½®çš„ use_full_dataset æ ‡å¿—ï¼Œå°†æ§åˆ¶æƒå®Œå…¨äº¤ç”±UIã€‚
"""

import gradio as gr
import torch
import os
import subprocess
import time
import json
from pathlib import Path
from threading import Thread

# æé«˜ PyTorch Dynamo çš„é‡ç¼–è¯‘å®¹å¿ä¸Šé™
torch._dynamo.config.recompile_limit = 100

from unsloth import FastLanguageModel
from transformers import TrainingArguments, TextIteratorStreamer
from transformers.integrations import TensorBoardCallback
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets, load_from_disk

# --- (å‰é¢çš„ä»£ç ï¼Œå¦‚å…¨å±€é…ç½®å’Œè¾…åŠ©å‡½æ•°ï¼Œä¿æŒä¸å˜) ---
# --- 1. å…¨å±€æ ¸å¿ƒé…ç½® ---
LOGS_PARENT_DIR = "logs"
OUTPUTS_PARENT_DIR = "outputs"
TENSORBOARD_PROC = None


# --- 2. è¾…åŠ©å‡½æ•°ï¼šé…ç½®åŠ è½½ ---
def load_config_from_json_files(config_path, default_config, error_msg):
    if isinstance(config_path, Path) and config_path.is_dir():
        json_files = list(config_path.glob("*.json"))
        if not json_files:
            return [default_config], [default_config.get("display_name", "Default")]
        configs = [json.load(open(f, "r", encoding="utf-8")) for f in json_files]
        return configs, [cfg["display_name"] for cfg in configs]
    elif Path(config_path).is_file():
        with open(config_path, "r", encoding="utf-8") as f:
            configs = json.load(f)
        return configs, [cfg["display_name"] for cfg in configs]
    else:
        print(f"è­¦å‘Š: {error_msg}")
        return [default_config], [default_config.get("display_name", "Default")]


MODELS_CONFIG, MODEL_DISPLAY_NAMES = load_config_from_json_files(
    Path("models.json"), {}, "æ¨¡å‹é…ç½®æ–‡ä»¶ 'models.json' æœªæ‰¾åˆ°ã€‚"
)
DATASETS_CONFIG, DATASET_DISPLAY_NAMES = load_config_from_json_files(
    Path("datasets_config"), {}, "æ•°æ®é›†é…ç½®ç›®å½• 'datasets_config' æœªæ‰¾åˆ°ã€‚"
)


# --- 3. è¾…åŠ©å‡½æ•°ï¼šå¯åŠ¨ TensorBoard ---
def launch_tensorboard():
    global TENSORBOARD_PROC
    if TENSORBOARD_PROC is not None and TENSORBOARD_PROC.poll() is None:
        return
    os.makedirs(LOGS_PARENT_DIR, exist_ok=True)
    TENSORBOARD_PROC = subprocess.Popen(
        [
            "tensorboard",
            "--logdir",
            LOGS_PARENT_DIR,
            "--host",
            "0.0.0.0",
            "--port",
            "6006",
        ]
    )
    time.sleep(5)


# --- 4. è¾…åŠ©å‡½æ•°ï¼šå‡†å¤‡æ•°æ®é›† (å·²æ›´æ–°) ---
def prepare_dataset(dataset_config, truncate_for_testing):
    """æ ¹æ®é…ç½®åŠ è½½æ•°æ®ï¼Œå¹¶æ ¹æ®UIé€‰é¡¹å†³å®šæ˜¯å¦æˆªæ–­ã€‚"""
    if dataset_config.get("is_local", False):
        dataset_path = dataset_config["dataset_id"]
        if not Path(dataset_path).exists():
            raise FileNotFoundError(f"æœ¬åœ°æ•°æ®é›†è·¯å¾„æœªæ‰¾åˆ°: {dataset_path}")
        dataset = load_from_disk(dataset_path)
    else:
        dataset = load_dataset(
            dataset_config["dataset_id"], split=dataset_config.get("split", "train")
        )

    prompt_template = dataset_config["prompt_template"]
    column_mappings = dataset_config["input_columns"]

    def formatting_prompts_func(examples):
        texts = []
        zipped_columns = zip(
            *(examples[col_name] for col_name in column_mappings.values())
        )
        for values in zipped_columns:
            format_dict = dict(zip(column_mappings.keys(), values))
            texts.append(prompt_template.format(**format_dict))
        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)

    # vvvvvvvvvvvvvv è¿™æ˜¯ä¿®æ”¹çš„åœ°æ–¹ vvvvvvvvvvvvvv
    # æ ¹æ® UI å¤é€‰æ¡†çš„å€¼æ¥å†³å®šæ˜¯å¦æˆªæ–­
    if truncate_for_testing and len(dataset) > 200:
        print("æˆªæ–­æ•°æ®é›†è‡³å‰200æ¡ç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚")
        dataset = dataset.select(range(200))
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return dataset


# --- 5. æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•° (å·²æ›´æ–°) ---
def train_model(
    # -- æ–°å¢ truncate_dataset å‚æ•° --
    experiment_name,
    resume_training,
    truncate_dataset,
    # -- å…¶ä»–å‚æ•° --
    training_mode,
    num_epochs,
    max_steps,
    save_steps,
    selected_model_name,
    selected_dataset_names,
    lora_r,
    lora_alpha,
    batch_size,
    grad_accum,
    lr,
    progress=gr.Progress(track_tqdm=True),
):
    # ... (å‰é¢çš„ä»£ç ä¸å˜) ...
    if not experiment_name or not experiment_name.strip():
        return "é”™è¯¯ï¼šå®éªŒåç§°ä¸èƒ½ä¸ºç©ºã€‚"
    experiment_name = experiment_name.strip().replace(" ", "_")
    output_dir = Path(OUTPUTS_PARENT_DIR) / experiment_name
    logging_dir = Path(LOGS_PARENT_DIR) / experiment_name
    progress(0, desc=f"å‡†å¤‡å®éªŒ: {experiment_name}")

    model_config = next(
        (item for item in MODELS_CONFIG if item["display_name"] == selected_model_name),
        None,
    )
    if not model_config or not selected_dataset_names:
        return "é”™è¯¯ï¼šå¿…é¡»é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œè‡³å°‘ä¸€ä¸ªæ•°æ®é›†ã€‚"
    progress(0.1, desc="åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")

    # vvvvvvvvvvvvvv è¿™æ˜¯ä¿®æ”¹çš„åœ°æ–¹ vvvvvvvvvvvvvv
    # å°† UI å¤é€‰æ¡†çš„å€¼ä¼ é€’ç»™ prepare_dataset å‡½æ•°
    all_datasets = [
        prepare_dataset(cfg, truncate_dataset)
        for name in selected_dataset_names
        if (
            cfg := next(
                (item for item in DATASETS_CONFIG if item["display_name"] == name), None
            )
        )
    ]
    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

    if not all_datasets:
        return "é”™è¯¯ï¼šæ— æ³•åŠ è½½æ‰€é€‰çš„æ•°æ®é›†é…ç½®ã€‚"
    progress(0.2, desc=f"åˆå¹¶ {len(all_datasets)} ä¸ªæ•°æ®é›†ä¸­...")
    combined_dataset = concatenate_datasets(all_datasets)

    # ... (åç»­è®­ç»ƒé€»è¾‘ä¸ä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒ) ...
    progress(0.3, desc=f"åˆå§‹åŒ–æ¨¡å‹: {model_config['model_id']}...")
    dtype_str = model_config.get("dtype")
    dtype = {"bfloat16": torch.bfloat16, "float16": torch.float16}.get(dtype_str)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config["model_id"],
        max_seq_length=2048,
        dtype=dtype,
        load_in_4bit=model_config["load_in_4bit"],
    )
    model = FastLanguageModel.get_peft_model(
        model,
        r=int(lora_r),
        lora_alpha=int(lora_alpha),
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_dropout=0,
        bias="none",
        use_gradient_checkpointing="unsloth",
        random_state=3407,
    )
    progress(0.4, desc="é…ç½®è®­ç»ƒå‚æ•°...")
    args = {
        "output_dir": str(output_dir),
        "logging_dir": str(logging_dir),
        "per_device_train_batch_size": int(batch_size),
        "gradient_accumulation_steps": int(grad_accum),
        "learning_rate": float(lr),
        "logging_steps": 1,
        "optim": "adamw_8bit",
        "fp16": not torch.cuda.is_bf16_supported(),
        "bf16": torch.cuda.is_bf16_supported(),
        "warmup_steps": 10,
        "weight_decay": 0.01,
        "lr_scheduler_type": "linear",
        "seed": 3407,
        "save_total_limit": 3,
    }
    if training_mode == "æŒ‰è½®æ¬¡ (Epochs)":
        args["num_train_epochs"] = float(num_epochs)
        args["save_strategy"] = "epoch"
    else:
        args["max_steps"] = int(max_steps)
        args["save_strategy"] = "steps"
        args["save_steps"] = int(save_steps)
    training_args = TrainingArguments(**args)
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=combined_dataset,
        dataset_text_field="text",
        max_seq_length=2048,
        dataset_num_proc=2,
        packing=False,
        args=training_args,
        callbacks=[TensorBoardCallback()],
    )
    status_msg = "ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ..." if resume_training else "å¼€å§‹æ–°çš„è®­ç»ƒ..."
    progress(0.5, desc=status_msg)
    trainer.train(resume_from_checkpoint=resume_training)
    progress(0.9, desc="è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆ LoRA é€‚é…å™¨...")
    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    progress(1.0, desc="æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    return f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆ LoRA é€‚é…å™¨å·²ä¿å­˜åœ¨ '{output_dir}'"


# --- 6. æ¨ç† (Inference) ç›¸å…³å‡½æ•° (ä¿æŒä¸å˜) ---
# ... (load_inference_model, run_chat ç­‰å‡½æ•°ä»£ç ä¸å˜) ...
def list_trained_loras():
    if not Path(OUTPUTS_PARENT_DIR).exists():
        return []
    return [d.name for d in Path(OUTPUTS_PARENT_DIR).iterdir() if d.is_dir()]


def load_inference_model(
    base_model_name, lora_name, progress=gr.Progress(track_tqdm=True)
):
    progress(0, desc="å¼€å§‹åŠ è½½æ¨¡å‹...")
    if not base_model_name or not lora_name:
        return None, None, "é”™è¯¯ï¼šå¿…é¡»åŒæ—¶é€‰æ‹©ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸€ä¸ª LoRA é€‚é…å™¨ã€‚"
    model_config = next(
        (item for item in MODELS_CONFIG if item["display_name"] == base_model_name),
        None,
    )
    if not model_config:
        return None, None, f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸºç¡€æ¨¡å‹ '{base_model_name}' çš„é…ç½®ã€‚"
    base_model_id = model_config["model_id"]
    lora_path = Path(OUTPUTS_PARENT_DIR) / lora_name
    if not lora_path.exists():
        return None, None, f"é”™è¯¯ï¼šLoRA ç›®å½•æœªæ‰¾åˆ°: {lora_path}"
    try:
        progress(0.2, desc=f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_id}...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=base_model_id, max_seq_length=2048, load_in_4bit=True
        )
        progress(0.6, desc=f"æ­£åœ¨åº”ç”¨ LoRA é€‚é…å™¨: {lora_name}...")
        model.load_adapter(str(lora_path))
        progress(1.0, desc="æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, f"âœ… æ¨¡å‹ '{lora_name}' åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚"
    except Exception as e:
        return None, None, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"


def run_chat(user_input, history, system_prompt, model, tokenizer):
    history = history or []
    if model is None or tokenizer is None:
        history.append({"role": "user", "content": user_input})
        history.append(
            {
                "role": "assistant",
                "content": "é”™è¯¯ï¼šæ¨¡å‹æœªåŠ è½½ã€‚è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹ã€‚",
            }
        )
        return history
    messages = (
        [{"role": "system", "content": system_prompt}]
        if system_prompt and system_prompt.strip()
        else []
    )
    messages.extend(history)
    messages.append({"role": "user", "content": user_input})
    prompt_inputs = tokenizer.apply_chat_template(
        messages, tokenize=True, add_generation_prompt=True, return_tensors="pt"
    ).to("cuda")
    streamer = TextIteratorStreamer(
        tokenizer, skip_prompt=True, skip_special_tokens=True
    )
    generation_kwargs = dict(
        input_ids=prompt_inputs,
        streamer=streamer,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.95,
        top_k=40,
        repetition_penalty=1.1,
        use_cache=True,
    )
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    history.append({"role": "assistant", "content": ""})
    for new_text in streamer:
        history[-1]["content"] += new_text
        yield history


# --- 7. Gradio UI ç•Œé¢å®šä¹‰ (å·²æ›´æ–°) ---
def update_training_mode_ui(mode):
    # ... (ä»£ç ä¸å˜) ...
    is_epoch_mode = mode == "æŒ‰è½®æ¬¡ (Epochs)"
    return {
        num_epochs_slider: gr.update(visible=is_epoch_mode),
        max_steps_slider: gr.update(visible=not is_epoch_mode),
        save_steps_input: gr.update(visible=not is_epoch_mode),
    }


with gr.Blocks(theme=gr.themes.Soft(), css="footer {display: none !important}") as demo:
    gr.Markdown("# ğŸš€ Unsloth GUI Trainer & Playground (v3.8)")

    loaded_model_state = gr.State(None)
    loaded_tokenizer_state = gr.State(None)

    with gr.Tabs():
        with gr.Tab("ğŸš€ è®­ç»ƒ (Train)"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## âš™ï¸ è®­ç»ƒé…ç½®")
                    with gr.Accordion("1. å®éªŒè®¾ç½®", open=True):
                        experiment_name_input = gr.Textbox(
                            label="å®éªŒåç§° (å¿…å¡«)", value="8gb-vram-test"
                        )
                        resume_checkbox = gr.Checkbox(
                            label="ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ", value=False
                        )
                    with gr.Accordion("2. æ¨¡å‹ä¸æ•°æ®é›†", open=True):
                        model_dropdown = gr.Dropdown(
                            choices=MODEL_DISPLAY_NAMES,
                            value=(
                                MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None
                            ),
                            label="é€‰æ‹©æ¨¡å‹",
                        )
                        dataset_dropdown = gr.Dropdown(
                            choices=DATASET_DISPLAY_NAMES,
                            value=(
                                [DATASET_DISPLAY_NAMES[0]]
                                if DATASET_DISPLAY_NAMES
                                else None
                            ),
                            label="é€‰æ‹©æ•°æ®é›† (å¯å¤šé€‰)",
                            multiselect=True,
                        )
                        # vvvvvvvvvvvvvv è¿™æ˜¯æ–°å¢çš„åœ°æ–¹ vvvvvvvvvvvvvv
                        truncate_dataset_checkbox = gr.Checkbox(
                            label="æˆªæ–­æ•°æ®é›†ç”¨äºå¿«é€Ÿæµ‹è¯• (ä»…ä½¿ç”¨å‰200æ¡)",
                            value=True,
                            info="å–æ¶ˆå‹¾é€‰ä»¥ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œæ­£å¼è®­ç»ƒã€‚",
                        )
                        # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                    with gr.Accordion("3. LoRA å‚æ•° (8GB ä¼˜åŒ–)", open=False):
                        lora_r_slider = gr.Slider(
                            4, 64, value=8, step=4, label="LoRA Rank (r)"
                        )
                        lora_alpha_slider = gr.Slider(
                            4, 128, value=16, step=4, label="LoRA Alpha"
                        )
                    with gr.Accordion("4. è®­ç»ƒæ ¸å¿ƒå‚æ•° (8GB ä¼˜åŒ–)", open=True):
                        training_mode_selector = gr.Radio(
                            ["æŒ‰æ­¥æ•° (Steps)", "æŒ‰è½®æ¬¡ (Epochs)"],
                            value="æŒ‰æ­¥æ•° (Steps)",
                            label="è®­ç»ƒæ¨¡å¼",
                        )
                        num_epochs_slider = gr.Slider(
                            0.1,
                            10,
                            value=1,
                            step=0.1,
                            label="è®­ç»ƒè½®æ•° (Epochs)",
                            visible=False,
                        )
                        max_steps_slider = gr.Slider(
                            10,
                            2000,
                            100,
                            step=10,
                            label="æœ€å¤§è®­ç»ƒæ­¥æ•° (Max Steps)",
                            visible=True,
                        )
                        save_steps_input = gr.Number(
                            value=50, label="æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡æ–­ç‚¹", visible=True
                        )
                        batch_size_slider = gr.Slider(
                            1, 16, value=1, step=1, label="Batch Size"
                        )
                        grad_accum_slider = gr.Slider(
                            1, 16, value=8, step=1, label="Gradient Accumulation"
                        )
                        learning_rate_slider = gr.Slider(
                            1e-5, 5e-4, 2e-4, step=1e-5, label="å­¦ä¹ ç‡"
                        )
                    start_button = gr.Button("âœ… å¼€å§‹è®­ç»ƒ", variant="primary")
                with gr.Column(scale=3):
                    gr.Markdown("## ğŸ“Š TensorBoard ç›‘æ§é¢æ¿")
                    tensorboard_html = f'<iframe src="http://127.0.0.1:6006" width="100%" height="1188px" frameborder="0"></iframe>'
                    tensorboard_view = gr.HTML(tensorboard_html)
            gr.Markdown("--- \n ## ğŸ“‹ è®­ç»ƒæ—¥å¿—ä¸çŠ¶æ€")
            status_output = gr.Textbox(
                label="Status", interactive=False, lines=3, max_lines=5
            )

        with gr.Tab("ğŸ’¬ æµ‹è¯• (Inference Playground)"):
            # ... (æµ‹è¯• Tab çš„ä»£ç ä¸å˜) ...
            gr.Markdown("## ğŸ§  ä¸ä½ è®­ç»ƒçš„æ¨¡å‹å¯¹è¯")
            gr.Markdown(
                "**æ­¥éª¤1**: é€‰æ‹©åŸºç¡€æ¨¡å‹ã€‚**æ­¥éª¤2**: é€‰æ‹© LoRA é€‚é…å™¨ã€‚**æ­¥éª¤3**: è®¾å®šç³»ç»Ÿæç¤ºã€‚"
            )
            with gr.Row():
                inference_model_selector = gr.Dropdown(
                    label="é€‰æ‹©åŸºç¡€æ¨¡å‹ (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´)",
                    choices=MODEL_DISPLAY_NAMES,
                    value=MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None,
                )
                lora_selector_dropdown = gr.Dropdown(
                    label="é€‰æ‹© LoRA é€‚é…å™¨", choices=list_trained_loras()
                )
            load_model_button = gr.Button("è½½å…¥æ¨¡å‹è¿›è¡Œæµ‹è¯•", variant="primary")
            load_status_textbox = gr.Textbox(label="æ¨¡å‹åŠ è½½çŠ¶æ€", interactive=False)
            system_prompt_textbox = gr.Textbox(
                label="ç³»ç»Ÿæç¤º (System Prompt)",
                info="ä¸ºä½ çš„ AI è®¾å®šä¸€ä¸ªèº«ä»½ã€è§„åˆ™æˆ–åŸºè°ƒã€‚",
                lines=3,
                value="ä½ ç°åœ¨æ˜¯é½å¤©å¤§åœ£å­™æ‚Ÿç©ºï¼Œè¯·ç”¨å­™æ‚Ÿç©ºçš„èº«ä»½å’Œé£æ ¼æ¥å›ç­”æ¥ä¸‹æ¥çš„æ‰€æœ‰é—®é¢˜ã€‚",
            )
            chatbot = gr.Chatbot(label="èŠå¤©çª—å£", type="messages", height=500)
            with gr.Row():
                chat_input_textbox = gr.Textbox(
                    show_label=False,
                    placeholder="è¾“å…¥ä½ çš„æ¶ˆæ¯...",
                    scale=4,
                    container=False,
                )
                submit_button = gr.Button("å‘é€", variant="primary", scale=1)

    training_mode_selector.change(
        fn=update_training_mode_ui,
        inputs=training_mode_selector,
        outputs=[num_epochs_slider, max_steps_slider, save_steps_input],
    )

    train_inputs = [
        experiment_name_input,
        resume_checkbox,
        truncate_dataset_checkbox,  # <-- å·²æ·»åŠ 
        training_mode_selector,
        num_epochs_slider,
        max_steps_slider,
        save_steps_input,
        model_dropdown,
        dataset_dropdown,
        lora_r_slider,
        lora_alpha_slider,
        batch_size_slider,
        grad_accum_slider,
        learning_rate_slider,
    ]
    start_button.click(fn=train_model, inputs=train_inputs, outputs=[status_output])

    load_model_button.click(
        fn=load_inference_model,
        inputs=[inference_model_selector, lora_selector_dropdown],
        outputs=[loaded_model_state, loaded_tokenizer_state, load_status_textbox],
    )
    submit_event = chat_input_textbox.submit(
        fn=run_chat,
        inputs=[
            chat_input_textbox,
            chatbot,
            system_prompt_textbox,
            loaded_model_state,
            loaded_tokenizer_state,
        ],
        outputs=[chatbot],
    )
    submit_event.then(lambda: gr.update(value=""), outputs=[chat_input_textbox])
    button_event = submit_button.click(
        fn=run_chat,
        inputs=[
            chat_input_textbox,
            chatbot,
            system_prompt_textbox,
            loaded_model_state,
            loaded_tokenizer_state,
        ],
        outputs=[chatbot],
    )
    button_event.then(lambda: gr.update(value=""), outputs=[chat_input_textbox])

    demo.load(fn=launch_tensorboard)

# --- 8. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    demo.launch(share=True)
