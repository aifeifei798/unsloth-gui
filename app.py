"""
=====================================================================================
ğŸš€ Unsloth + Gradio + TensorBoard: ä¸€ä¸ªä¸“ä¸šçš„äº¤äº’å¼å¾®è°ƒå·¥ä½œå° (ç»ˆæç‰ˆ) ğŸš€
=====================================================================================
æ­¤ç‰ˆæœ¬é»˜è®¤é…ç½®å·²ä¸º 8GB VRAM æ˜¾å¡ (å¦‚ RTX 3070) è¿›è¡Œä¼˜åŒ–ï¼Œä»¥é˜²æ­¢å†…å­˜æº¢å‡ºã€‚
"""

import gradio as gr
import torch
import os
import subprocess
import time
import json
from pathlib import Path
from unsloth import FastLanguageModel
from transformers import TrainingArguments
from transformers.integrations import TensorBoardCallback
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets

# --- 1. å…¨å±€æ ¸å¿ƒé…ç½® ---
LOGS_PARENT_DIR = "logs"
OUTPUTS_PARENT_DIR = "outputs"
TENSORBOARD_PROC = None

# --- 2. è¾…åŠ©å‡½æ•°ï¼šé…ç½®åŠ è½½ (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ) ---
def load_config_from_json_files(config_path, default_config, error_msg):
    if isinstance(config_path, Path) and config_path.is_dir():
        json_files = list(config_path.glob("*.json"))
        if not json_files: return [default_config], [default_config.get('display_name', 'Default')]
        configs = [json.load(open(f, 'r', encoding='utf-8')) for f in json_files]
        return configs, [cfg['display_name'] for cfg in configs]
    elif Path(config_path).is_file():
        with open(config_path, 'r', encoding='utf-8') as f: configs = json.load(f)
        return configs, [cfg['display_name'] for cfg in configs]
    else:
        print(f"è­¦å‘Š: {error_msg}")
        return [default_config], [default_config.get('display_name', 'Default')]

MODELS_CONFIG, MODEL_DISPLAY_NAMES = load_config_from_json_files(Path("models.json"), {}, "æ¨¡å‹é…ç½®æ–‡ä»¶ 'models.json' æœªæ‰¾åˆ°ã€‚")
DATASETS_CONFIG, DATASET_DISPLAY_NAMES = load_config_from_json_files(Path("datasets_config"), {}, "æ•°æ®é›†é…ç½®ç›®å½• 'datasets_config' æœªæ‰¾åˆ°ã€‚")

# --- 3. è¾…åŠ©å‡½æ•°ï¼šå¯åŠ¨ TensorBoard (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ) ---
def launch_tensorboard():
    global TENSORBOARD_PROC
    if TENSORBOARD_PROC is not None and TENSORBOARD_PROC.poll() is None: return
    os.makedirs(LOGS_PARENT_DIR, exist_ok=True)
    TENSORBOARD_PROC = subprocess.Popen(['tensorboard', '--logdir', LOGS_PARENT_DIR, '--host', '0.0.0.0', '--port', '6006'])
    time.sleep(5)

# --- 4. è¾…åŠ©å‡½æ•°ï¼šå‡†å¤‡æ•°æ®é›† (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ) ---
def prepare_dataset(dataset_config):
    dataset = load_dataset(dataset_config['dataset_id'], split=dataset_config['split'])
    prompt_template = dataset_config['prompt_template']
    column_mappings = dataset_config['input_columns']
    def formatting_prompts_func(examples):
        texts = []
        zipped_columns = zip(*(examples[col_name] for col_name in column_mappings.values()))
        for values in zipped_columns:
            format_dict = dict(zip(column_mappings.keys(), values))
            texts.append(prompt_template.format(**format_dict))
        return {"text": texts}
    dataset = dataset.map(formatting_prompts_func, batched=True)
    # ä¸ºåŠ é€Ÿæ¼”ç¤ºï¼Œä»…ä½¿ç”¨å‰200ä¸ªæ ·æœ¬ã€‚åœ¨å®é™…ä½¿ç”¨ä¸­å¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œã€‚
    return dataset.select(range(200))

# --- 5. æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨¡å‹è®­ç»ƒå‡½æ•° (ä¸ä¹‹å‰ç‰ˆæœ¬ç›¸åŒ) ---
def train_model(
    experiment_name, resume_training,
    training_mode, num_epochs, max_steps, save_steps,
    selected_model_name, selected_dataset_names,
    lora_r, lora_alpha,
    batch_size, grad_accum, optimizer, lr,
    progress=gr.Progress(track_tqdm=True)
):
    if not experiment_name or not experiment_name.strip(): return "é”™è¯¯ï¼šå®éªŒåç§°ä¸èƒ½ä¸ºç©ºã€‚"
    experiment_name = experiment_name.strip().replace(" ", "_")
    output_dir = Path(OUTPUTS_PARENT_DIR) / experiment_name
    logging_dir = Path(LOGS_PARENT_DIR) / experiment_name
    progress(0, desc=f"å‡†å¤‡å®éªŒ: {experiment_name}")

    model_config = next((item for item in MODELS_CONFIG if item["display_name"] == selected_model_name), None)
    if not model_config or not selected_dataset_names: return "é”™è¯¯ï¼šå¿…é¡»é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œè‡³å°‘ä¸€ä¸ªæ•°æ®é›†ã€‚"
    progress(0.1, desc="åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")
    all_datasets = [prepare_dataset(cfg) for name in selected_dataset_names if (cfg := next((item for item in DATASETS_CONFIG if item["display_name"] == name), None))]
    if not all_datasets: return "é”™è¯¯ï¼šæ— æ³•åŠ è½½æ‰€é€‰çš„æ•°æ®é›†é…ç½®ã€‚"
    progress(0.2, desc=f"åˆå¹¶ {len(all_datasets)} ä¸ªæ•°æ®é›†ä¸­...")
    combined_dataset = concatenate_datasets(all_datasets)

    progress(0.3, desc=f"åˆå§‹åŒ–æ¨¡å‹: {model_config['model_id']}...")
    dtype_str = model_config.get('dtype')
    dtype = {'bfloat16': torch.bfloat16, 'float16': torch.float16}.get(dtype_str)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config['model_id'], max_seq_length=2048, dtype=dtype, load_in_4bit=model_config['load_in_4bit'],
    )
    model = FastLanguageModel.get_peft_model(
        model, r=int(lora_r), lora_alpha=int(lora_alpha),
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0, bias="none", use_gradient_checkpointing="unslued", random_state=3407,
    )

    progress(0.4, desc="é…ç½®è®­ç»ƒå‚æ•°...")
    args = {
        "output_dir": str(output_dir), "logging_dir": str(logging_dir),
        "per_device_train_batch_size": int(batch_size), "gradient_accumulation_steps": int(grad_accum),
        "learning_rate": float(lr), "logging_steps": 1, "optim": optimizer,
        "fp16": not torch.cuda.is_bf16_supported(), "bf16": torch.cuda.is_bf16_supported(),
        "warmup_steps": 10, "weight_decay": 0.01, "lr_scheduler_type": "linear",
        "seed": 3407, "save_total_limit": 3,
    }

    if training_mode == "æŒ‰è½®æ¬¡ (Epochs)":
        args["num_train_epochs"] = float(num_epochs)
        args["save_strategy"] = "epoch"
    else:
        args["max_steps"] = int(max_steps)
        args["save_strategy"] = "steps"
        args["save_steps"] = int(save_steps)
        
    training_args = TrainingArguments(**args)

    trainer = SFTTrainer(
        model=model, tokenizer=tokenizer, train_dataset=combined_dataset,
        dataset_text_field="text", max_seq_length=2048, dataset_num_proc=2,
        packing=False, args=training_args, callbacks=[TensorBoardCallback()],
    )

    status_msg = "ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ..." if resume_training else "å¼€å§‹æ–°çš„è®­ç»ƒ..."
    progress(0.5, desc=status_msg)
    trainer.train(resume_from_checkpoint=resume_training)
    
    progress(1.0, desc="è®­ç»ƒå®Œæˆï¼")
    return f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åœ¨ '{output_dir}'"

# --- 6. Gradio UI ç•Œé¢å®šä¹‰ (å·²æ›´æ–°é»˜è®¤å€¼) ---
def update_training_mode_ui(mode):
    is_epoch_mode = mode == "æŒ‰è½®æ¬¡ (Epochs)"
    return {
        num_epochs_slider: gr.update(visible=is_epoch_mode),
        max_steps_slider: gr.update(visible=not is_epoch_mode),
        save_steps_input: gr.update(visible=not is_epoch_mode),
    }

with gr.Blocks(theme=gr.themes.Soft(), css="footer {display: none !important}") as demo:
    gr.Markdown("# ğŸš€ Unsloth + Gradio + TensorBoard (ç»ˆæç‰ˆ)")
    gr.Markdown("### âœ¨ *é»˜è®¤å‚æ•°å·²ä¸º 8GB VRAM (å¦‚ RTX 3070) ä¼˜åŒ–*")
    
    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("## âš™ï¸ è®­ç»ƒé…ç½®")
            with gr.Accordion("1. å®éªŒè®¾ç½®", open=True):
                experiment_name_input = gr.Textbox(label="å®éªŒåç§° (å¿…å¡«)", value="3070-8gb-default-test")
                resume_checkbox = gr.Checkbox(label="ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ", value=False)
            
            with gr.Accordion("2. æ¨¡å‹ä¸æ•°æ®é›†", open=True):
                model_dropdown = gr.Dropdown(choices=MODEL_DISPLAY_NAMES, value=MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None, label="é€‰æ‹©æ¨¡å‹")
                dataset_dropdown = gr.Dropdown(choices=DATASET_DISPLAY_NAMES, value=[DATASET_DISPLAY_NAMES[0]] if DATASET_DISPLAY_NAMES else None, label="é€‰æ‹©æ•°æ®é›† (å¯å¤šé€‰)", multiselect=True)

            with gr.Accordion("3. LoRA å‚æ•° (8GB ä¼˜åŒ–)", open=False):
                lora_r_slider = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank (r)") # <<< 8GB ä¼˜åŒ–
                lora_alpha_slider = gr.Slider(4, 128, value=16, step=4, label="LoRA Alpha") # <<< 8GB ä¼˜åŒ–

            with gr.Accordion("4. è®­ç»ƒæ ¸å¿ƒå‚æ•° (8GB ä¼˜åŒ–)", open=True):
                training_mode_selector = gr.Radio(["æŒ‰æ­¥æ•° (Steps)", "æŒ‰è½®æ¬¡ (Epochs)"], value="æŒ‰æ­¥æ•° (Steps)", label="è®­ç»ƒæ¨¡å¼")
                num_epochs_slider = gr.Slider(0.1, 10, 3, step=0.1, label="è®­ç»ƒè½®æ•° (Epochs)", visible=False)
                max_steps_slider = gr.Slider(10, 2000, 100, step=10, label="æœ€å¤§è®­ç»ƒæ­¥æ•° (Max Steps)", visible=True)
                save_steps_input = gr.Number(value=50, label="æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡æ–­ç‚¹", visible=True)
                
                batch_size_slider = gr.Slider(1, 16, value=1, step=1, label="Batch Size") # <<< 8GB ä¼˜åŒ–
                grad_accum_slider = gr.Slider(1, 16, value=8, step=1, label="Gradient Accumulation") # <<< 8GB ä¼˜åŒ–
                optimizer_dropdown = gr.Dropdown(["adamw_8bit", "adamw_torch"], value="adamw_8bit", label="ä¼˜åŒ–å™¨")
                learning_rate_slider = gr.Slider(1e-5, 5e-4, 2e-4, step=1e-5, label="å­¦ä¹ ç‡")

            start_button = gr.Button("âœ… å¼€å§‹è®­ç»ƒ", variant="primary")
            status_output = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", interactive=False, lines=2)

        with gr.Column(scale=3):
            gr.Markdown("## ğŸ“Š TensorBoard ç›‘æ§é¢æ¿")
            tensorboard_html = f'<iframe src="http://127.0.0.1:6006" width="100%" height="800px" frameborder="0"></iframe>'
            tensorboard_view = gr.HTML(tensorboard_html)

    training_mode_selector.change(fn=update_training_mode_ui, inputs=training_mode_selector, outputs=[num_epochs_slider, max_steps_slider, save_steps_input])
    
    all_inputs = [
        experiment_name_input, resume_checkbox,
        training_mode_selector, num_epochs_slider, max_steps_slider, save_steps_input,
        model_dropdown, dataset_dropdown, lora_r_slider, lora_alpha_slider,
        batch_size_slider, grad_accum_slider, optimizer_dropdown, learning_rate_slider
    ]
    start_button.click(fn=train_model, inputs=all_inputs, outputs=[status_output])
    
    demo.load(fn=launch_tensorboard)

# --- 7. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    demo.launch(share=True)