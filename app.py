"""
=====================================================================================
ğŸš€ Unsloth GUI Trainer & Playground (v3.6 - æœ€ç»ˆç¨³å®šç‰ˆ) ğŸš€
=====================================================================================

æœ¬åº”ç”¨æ˜¯ä¸€ä¸ªä¸€ç«™å¼çš„ã€å›¾å½¢åŒ–çš„ LLaMA & Mistral æ¨¡å‹ LoRA å¾®è°ƒå·¥ä½œå°ï¼Œ
æ—¨åœ¨å°†åŸæœ¬å¤æ‚çš„æ¨¡å‹å¾®è°ƒè¿‡ç¨‹ï¼Œç®€åŒ–åˆ°äººäººå¯ç”¨çš„ç¨‹åº¦ã€‚

æ ¸å¿ƒåŠŸèƒ½:
1.  **é…ç½®é©±åŠ¨**: é€šè¿‡ JSON æ–‡ä»¶åŠ¨æ€ç®¡ç†æ¨¡å‹ä¸æ•°æ®é›†ï¼Œæ˜“äºæ‰©å±•ã€‚
2.  **åŒæºæ•°æ®**: æ”¯æŒä» Hugging Face Hub åœ¨çº¿ä¸‹è½½ï¼Œæˆ–ä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†ã€‚
3.  **çµæ´»è®­ç»ƒ**: æ”¯æŒæŒ‰â€œè½®æ¬¡(Epoch)â€æˆ–â€œæ­¥æ•°(Step)â€ä¸¤ç§æ¨¡å¼ï¼Œå¹¶æ”¯æŒæ–­ç‚¹ç»­è®­ã€‚
4.  **å®æ—¶ç›‘æ§**: é›†æˆ TensorBoardï¼Œè®­ç»ƒè¿‡ç¨‹å…¨ç¨‹å¯è§†åŒ–ã€‚
5.  **ä¸€é”®äº§å‡º**: è®­ç»ƒå®Œæˆåè‡ªåŠ¨ä¿å­˜å¯ç›´æ¥ç”¨äºæ¨ç†çš„ LoRA é€‚é…å™¨ã€‚
6.  **å³æ—¶æµ‹è¯•**: å†…ç½®â€œæ¨ç†æ¸¸ä¹åœºâ€ï¼Œå¯åŠ è½½ä»»ä½•è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹ï¼Œ
    å¹¶é€šè¿‡â€œç³»ç»Ÿæç¤ºâ€è¿›è¡Œå¤šè½®ã€æµå¼çš„å¯¹è¯æµ‹è¯•ã€‚
7.  **ç¡¬ä»¶ä¼˜åŒ–**: é»˜è®¤å‚æ•°å·²ä¸º 8GB VRAM æ¶ˆè´¹çº§æ˜¾å¡ä¼˜åŒ–ï¼Œå¼€ç®±å³ç”¨ã€‚

æ–‡ä»¶ç»“æ„è¦æ±‚:
/
|-- app.py                 (æœ¬è„šæœ¬)
|-- models.json            (æ¨¡å‹é…ç½®æ–‡ä»¶)
|-- datasets_config/       (å­˜æ”¾æ•°æ®é›†é…ç½®çš„ç›®å½•)
|-- outputs/               (å­˜æ”¾æ‰€æœ‰æ¨¡å‹è¾“å‡ºå’Œæ–­ç‚¹çš„çˆ¶ç›®å½•)
|-- logs/                  (å­˜æ”¾æ‰€æœ‰ TensorBoard æ—¥å¿—çš„çˆ¶ç›®å½•)

=====================================================================================
"""

# --- 0. æ ¸å¿ƒåº“å¯¼å…¥ä¸å…¨å±€é…ç½® ---

import gradio as gr
import torch
import os
import subprocess
import time
import json
from pathlib import Path
from threading import Thread

# æé«˜ PyTorch Dynamo çš„é‡ç¼–è¯‘å®¹å¿ä¸Šé™ï¼Œä»¥é¿å…å› å¯å˜æ•°æ®å½¢çŠ¶å¯¼è‡´çš„æŠ¥é”™ã€‚
# è¿™å¯¹äº Gemma3 ç­‰æ–°æ¨¡å‹åœ¨å¤„ç†ä¸åŒé•¿åº¦åºåˆ—æ—¶å°¤å…¶é‡è¦ã€‚
torch._dynamo.config.recompile_limit = 100

# Unsloth ä¸ Hugging Face æ ¸å¿ƒç»„ä»¶
from unsloth import FastLanguageModel
from transformers import TrainingArguments, TextIteratorStreamer
from transformers.integrations import TensorBoardCallback
from trl import SFTTrainer
from datasets import load_dataset, concatenate_datasets, load_from_disk

# å…¨å±€å˜é‡å®šä¹‰
LOGS_PARENT_DIR = "logs"          # æ‰€æœ‰ TensorBoard æ—¥å¿—çš„çˆ¶ç›®å½•
OUTPUTS_PARENT_DIR = "outputs"      # æ‰€æœ‰æ¨¡å‹è¾“å‡ºå’Œæ–­ç‚¹çš„çˆ¶ç›®å½•
TENSORBOARD_PROC = None             # ç”¨äºæŒæœ‰ TensorBoard çš„åå°è¿›ç¨‹


# --- 1. è¾…åŠ©å‡½æ•° ---

def load_config_from_json_files(config_path, default_config, error_msg):
    """ä¸€ä¸ªé€šç”¨çš„é…ç½®åŠ è½½å‡½æ•°ï¼Œæ™ºèƒ½å¤„ç†å•ä¸ªæ–‡ä»¶æˆ–ç›®å½•ã€‚"""
    if isinstance(config_path, Path) and config_path.is_dir():
        json_files = list(config_path.glob("*.json"))
        if not json_files:
            print(f"è­¦å‘Š: ç›®å½• '{config_path}' ä¸ºç©ºæˆ–ä¸åŒ…å« .json æ–‡ä»¶ã€‚")
            return [default_config], [default_config.get('display_name', 'Default')]
        configs = [json.load(open(f, 'r', encoding='utf-8')) for f in json_files]
        return configs, [cfg['display_name'] for cfg in configs]
    elif Path(config_path).is_file():
        with open(config_path, 'r', encoding='utf-8') as f:
            configs = json.load(f)
        return configs, [cfg['display_name'] for cfg in configs]
    else:
        print(f"è­¦å‘Š: {error_msg}")
        return [default_config], [default_config.get('display_name', 'Default')]

def launch_tensorboard():
    """åœ¨åå°å¯åŠ¨ TensorBoard æœåŠ¡ï¼Œç›‘æ§æ‰€æœ‰å®éªŒçš„çˆ¶æ—¥å¿—ç›®å½•ã€‚"""
    global TENSORBOARD_PROC
    if TENSORBOARD_PROC is not None and TENSORBOARD_PROC.poll() is None:
        return
    os.makedirs(LOGS_PARENT_DIR, exist_ok=True)
    TENSORBOARD_PROC = subprocess.Popen([
        'tensorboard', '--logdir', LOGS_PARENT_DIR, '--host', '0.0.0.0', '--port', '6006'
    ])
    time.sleep(5)  # ç­‰å¾… TensorBoard å¯åŠ¨

# --- 2. æ•°æ®å¤„ç†æ ¸å¿ƒ ---

def prepare_dataset(dataset_config):
    """
    æ ¹æ®å•ä¸ªæ•°æ®é›†çš„é…ç½®ï¼Œæ™ºèƒ½åœ°åŠ è½½å¹¶æ ¼å¼åŒ–æ•°æ®ã€‚
    """
    # æ­¥éª¤ 1: æ ¹æ® is_local æ ‡å¿—ï¼Œä»æœ¬åœ°æˆ– Hub åŠ è½½
    if dataset_config.get("is_local", False):
        dataset_path = dataset_config['dataset_id']
        if not Path(dataset_path).exists():
            raise FileNotFoundError(f"æœ¬åœ°æ•°æ®é›†è·¯å¾„æœªæ‰¾åˆ°: {dataset_path}")
        dataset = load_from_disk(dataset_path)
    else:
        dataset = load_dataset(dataset_config['dataset_id'], split=dataset_config.get('split', 'train'))

    # æ­¥éª¤ 2: æ ¹æ®æ¨¡æ¿å’Œåˆ—æ˜ å°„ï¼Œæ ¼å¼åŒ–æ•°æ®é›†
    prompt_template = dataset_config['prompt_template']
    column_mappings = dataset_config['input_columns']
    
    def formatting_prompts_func(examples):
        texts = []
        zipped_columns = zip(*(examples[col_name] for col_name in column_mappings.values()))
        for values in zipped_columns:
            format_dict = dict(zip(column_mappings.keys(), values))
            texts.append(prompt_template.format(**format_dict))
        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)
    
    # ä¸ºåŠ é€Ÿæ¼”ç¤ºï¼Œé»˜è®¤ä»…ä½¿ç”¨å‰200ä¸ªæ ·æœ¬ã€‚å¯åœ¨é…ç½®ä¸­é€šè¿‡ "use_full_dataset": true å…³é—­ã€‚
    if len(dataset) > 200 and not dataset_config.get("use_full_dataset", False):
        dataset = dataset.select(range(200))
    return dataset


# --- 3. è®­ç»ƒä¸æ¨ç†æ ¸å¿ƒ ---

def train_model(
    # -- è¾“å…¥å‚æ•° --
    experiment_name, resume_training, training_mode, num_epochs, max_steps, save_steps,
    selected_model_name, selected_dataset_names, lora_r, lora_alpha,
    batch_size, grad_accum, optimizer, lr,
    progress=gr.Progress(track_tqdm=True)
):
    """æ¥æ”¶æ‰€æœ‰UIå‚æ•°å¹¶æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹å¾®è°ƒæµç¨‹ã€‚"""
    # 1. è®¾ç½®å®éªŒè·¯å¾„
    if not experiment_name or not experiment_name.strip(): return "é”™è¯¯ï¼šå®éªŒåç§°ä¸èƒ½ä¸ºç©ºã€‚"
    experiment_name = experiment_name.strip().replace(" ", "_")
    output_dir = Path(OUTPUTS_PARENT_DIR) / experiment_name
    logging_dir = Path(LOGS_PARENT_DIR) / experiment_name
    progress(0, desc=f"å‡†å¤‡å®éªŒ: {experiment_name}")

    # 2. åŠ è½½å¹¶åˆå¹¶æ•°æ®é›†
    model_config = next((item for item in MODELS_CONFIG if item["display_name"] == selected_model_name), None)
    if not model_config or not selected_dataset_names: return "é”™è¯¯ï¼šå¿…é¡»é€‰æ‹©ä¸€ä¸ªæ¨¡å‹å’Œè‡³å°‘ä¸€ä¸ªæ•°æ®é›†ã€‚"
    
    progress(0.1, desc="åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")
    all_datasets = [prepare_dataset(cfg) for name in selected_dataset_names if (cfg := next((item for item in DATASETS_CONFIG if item["display_name"] == name), None))]
    if not all_datasets: return "é”™è¯¯ï¼šæ— æ³•åŠ è½½æ‰€é€‰çš„æ•°æ®é›†é…ç½®ã€‚"
    
    progress(0.2, desc=f"åˆå¹¶ {len(all_datasets)} ä¸ªæ•°æ®é›†ä¸­...")
    combined_dataset = concatenate_datasets(all_datasets)

    # 3. åˆå§‹åŒ–æ¨¡å‹å’Œ Tokenizer
    progress(0.3, desc=f"åˆå§‹åŒ–æ¨¡å‹: {model_config['model_id']}...")
    dtype_str = model_config.get('dtype')
    dtype = {'bfloat16': torch.bfloat16, 'float16': torch.float16}.get(dtype_str)
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=model_config['model_id'], max_seq_length=2048, dtype=dtype, load_in_4bit=model_config['load_in_4bit'],
    )
    
    # 4. åº”ç”¨ LoRA (PEFT) é…ç½®
    model = FastLanguageModel.get_peft_model(
        model, r=int(lora_r), lora_alpha=int(lora_alpha),
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0, bias="none", use_gradient_checkpointing="unsloth", random_state=3407,
    )
    
    # 5. é…ç½®è®­ç»ƒå‚æ•° (TrainingArguments)
    progress(0.4, desc="é…ç½®è®­ç»ƒå‚æ•°...")
    args = {
        "output_dir": str(output_dir), "logging_dir": str(logging_dir),
        "per_device_train_batch_size": int(batch_size), "gradient_accumulation_steps": int(grad_accum),
        "learning_rate": float(lr), "logging_steps": 1, "optim": optimizer,
        "fp16": not torch.cuda.is_bf16_supported(), "bf16": torch.cuda.is_bf16_supported(),
        "warmup_steps": 10, "weight_decay": 0.01, "lr_scheduler_type": "linear",
        "seed": 3407, "save_total_limit": 3,
    }
    if training_mode == "æŒ‰è½®æ¬¡ (Epochs)":
        args["num_train_epochs"] = float(num_epochs)
        args["save_strategy"] = "epoch"
    else:
        args["max_steps"] = int(max_steps)
        args["save_strategy"] = "steps"
        args["save_steps"] = int(save_steps)
    training_args = TrainingArguments(**args)

    # 6. åˆå§‹åŒ–å¹¶å¼€å§‹è®­ç»ƒ
    trainer = SFTTrainer(
        model=model, tokenizer=tokenizer, train_dataset=combined_dataset,
        dataset_text_field="text", max_seq_length=2048, dataset_num_proc=2,
        packing=False, args=training_args, callbacks=[TensorBoardCallback()],
    )
    status_msg = "ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ..." if resume_training else "å¼€å§‹æ–°çš„è®­ç»ƒ..."
    progress(0.5, desc=status_msg)
    trainer.train(resume_from_checkpoint=resume_training)
    
    # 7. è®­ç»ƒå®Œæˆåï¼Œä¿å­˜æœ€ç»ˆçš„ã€å¯ç”¨äºæ¨ç†çš„ LoRA é€‚é…å™¨
    progress(0.9, desc="è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜æœ€ç»ˆ LoRA é€‚é…å™¨...")
    model.save_pretrained(str(output_dir))
    tokenizer.save_pretrained(str(output_dir))
    
    progress(1.0, desc="æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")
    return f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆ LoRA é€‚é…å™¨å·²ä¿å­˜åœ¨ '{output_dir}'"

def list_trained_loras():
    """æ‰«æ outputs ç›®å½•ï¼Œè¿”å›æ‰€æœ‰è®­ç»ƒå¥½çš„ LoRA é€‚é…å™¨ï¼ˆå®éªŒï¼‰åˆ—è¡¨ã€‚"""
    if not Path(OUTPUTS_PARENT_DIR).exists(): return []
    return [d.name for d in Path(OUTPUTS_PARENT_DIR).iterdir() if d.is_dir()]

def load_inference_model(base_model_name, lora_name, progress=gr.Progress(track_tqdm=True)):
    """åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨æŒ‡å®šçš„ LoRA é€‚é…å™¨ã€‚"""
    progress(0, desc="å¼€å§‹åŠ è½½æ¨¡å‹...")
    if not base_model_name or not lora_name:
        return None, None, "é”™è¯¯ï¼šå¿…é¡»åŒæ—¶é€‰æ‹©ä¸€ä¸ªåŸºç¡€æ¨¡å‹å’Œä¸€ä¸ª LoRA é€‚é…å™¨ã€‚"
    
    model_config = next((item for item in MODELS_CONFIG if item["display_name"] == base_model_name), None)
    if not model_config:
        return None, None, f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸºç¡€æ¨¡å‹ '{base_model_name}' çš„é…ç½®ã€‚"
    base_model_id = model_config['model_id']

    lora_path = Path(OUTPUTS_PARENT_DIR) / lora_name
    if not lora_path.exists():
        return None, None, f"é”™è¯¯ï¼šLoRA ç›®å½•æœªæ‰¾åˆ°: {lora_path}"
    
    try:
        progress(0.2, desc=f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_id}...")
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=base_model_id, max_seq_length=2048, load_in_4bit=True,
        )
        progress(0.6, desc=f"æ­£åœ¨åº”ç”¨ LoRA é€‚é…å™¨: {lora_name}...")
        model.load_adapter(str(lora_path))

        progress(1.0, desc="æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer, f"âœ… æ¨¡å‹ '{lora_name}' åŠ è½½æˆåŠŸï¼å¯ä»¥å¼€å§‹å¯¹è¯äº†ã€‚"
    except Exception as e:
        return None, None, f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}"

def run_chat(user_input, history, system_prompt, model, tokenizer):
    """å¤„ç†èŠå¤©è¾“å…¥ï¼Œæ”¯æŒç³»ç»Ÿæç¤ºï¼Œå¹¶ä»¥çœŸæ­£çš„æµå¼æ–¹å¼è¿”å›æ¨¡å‹å“åº”ã€‚"""
    history = history or []
    if model is None or tokenizer is None:
        history.append({"role": "user", "content": user_input})
        history.append({"role": "assistant", "content": "é”™è¯¯ï¼šæ¨¡å‹æœªåŠ è½½ã€‚è¯·å…ˆé€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªæ¨¡å‹ã€‚"})
        return history

    # æ„å»ºåŒ…å«ç³»ç»Ÿæç¤ºçš„ messages åˆ—è¡¨
    messages = [{"role": "system", "content": system_prompt}] if system_prompt and system_prompt.strip() else []
    messages.extend(history)
    messages.append({"role": "user", "content": user_input})
    
    # åˆå§‹åŒ–æµå¼è¾“å‡ºå™¨
    prompt_inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt").to("cuda")
    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    generation_kwargs = dict(input_ids=prompt_inputs, streamer=streamer, max_new_tokens=256,
        do_sample=True, temperature=0.7, top_p=0.95, top_k=40, repetition_penalty=1.1, use_cache=True)
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆï¼Œä»¥é¿å…UIé˜»å¡
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # å®æ—¶ yield æ–°ç”Ÿæˆçš„æ–‡æœ¬
    history.append({"role": "assistant", "content": ""})
    for new_text in streamer:
        history[-1]["content"] += new_text
        yield history


# --- 4. Gradio UI ç•Œé¢å®šä¹‰ ---

# åœ¨ç¨‹åºå¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰é…ç½®
MODELS_CONFIG, MODEL_DISPLAY_NAMES = load_config_from_json_files(Path("models.json"), {}, "æ¨¡å‹é…ç½®æ–‡ä»¶ 'models.json' æœªæ‰¾åˆ°ã€‚")
DATASETS_CONFIG, DATASET_DISPLAY_NAMES = load_config_from_json_files(Path("datasets_config"), {}, "æ•°æ®é›†é…ç½®ç›®å½• 'datasets_config' æœªæ‰¾åˆ°ã€‚")

def update_training_mode_ui(mode):
    """æ ¹æ®é€‰æ‹©çš„è®­ç»ƒæ¨¡å¼ï¼ŒåŠ¨æ€æ›´æ–°UIå…ƒç´ çš„å¯è§æ€§ã€‚"""
    is_epoch_mode = mode == "æŒ‰è½®æ¬¡ (Epochs)"
    return {
        num_epochs_slider: gr.update(visible=is_epoch_mode),
        max_steps_slider: gr.update(visible=not is_epoch_mode),
        save_steps_input: gr.update(visible=not is_epoch_mode),
    }

with gr.Blocks(theme=gr.themes.Soft(), css="footer {display: none !important}") as demo:
    gr.Markdown("# ğŸš€ Unsloth GUI Trainer & Playground (v3.6)")
    
    # ç”¨äºåœ¨ä¼šè¯ä¸­å­˜å‚¨åŠ è½½å¥½çš„æ¨¡å‹å’Œåˆ†è¯å™¨çš„çŠ¶æ€å˜é‡
    loaded_model_state = gr.State(None)
    loaded_tokenizer_state = gr.State(None)
    
    with gr.Tabs():
        # ========================== è®­ç»ƒé€‰é¡¹å¡ ==========================
        with gr.Tab("ğŸš€ è®­ç»ƒ (Train)"):
            with gr.Row():
                # -- å·¦ä¾§ï¼šé…ç½®é¢æ¿ --
                with gr.Column(scale=1):
                    gr.Markdown("## âš™ï¸ è®­ç»ƒé…ç½®")
                    with gr.Accordion("1. å®éªŒè®¾ç½®", open=True):
                        experiment_name_input = gr.Textbox(label="å®éªŒåç§° (å¿…å¡«)", value="8gb-vram-test")
                        resume_checkbox = gr.Checkbox(label="ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ", value=False)
                    with gr.Accordion("2. æ¨¡å‹ä¸æ•°æ®é›†", open=True):
                        model_dropdown = gr.Dropdown(choices=MODEL_DISPLAY_NAMES, value=MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None, label="é€‰æ‹©æ¨¡å‹")
                        dataset_dropdown = gr.Dropdown(choices=DATASET_DISPLAY_NAMES, value=[DATASET_DISPLAY_NAMES[0]] if DATASET_DISPLAY_NAMES else None, label="é€‰æ‹©æ•°æ®é›† (å¯å¤šé€‰)", multiselect=True)
                    with gr.Accordion("3. LoRA å‚æ•° (8GB ä¼˜åŒ–)", open=False):
                        lora_r_slider = gr.Slider(4, 64, value=8, step=4, label="LoRA Rank (r)")
                        lora_alpha_slider = gr.Slider(4, 128, value=16, step=4, label="LoRA Alpha")
                    with gr.Accordion("4. è®­ç»ƒæ ¸å¿ƒå‚æ•° (8GB ä¼˜åŒ–)", open=True):
                        training_mode_selector = gr.Radio(["æŒ‰æ­¥æ•° (Steps)", "æŒ‰è½®æ¬¡ (Epochs)"], value="æŒ‰æ­¥æ•° (Steps)", label="è®­ç»ƒæ¨¡å¼")
                        num_epochs_slider = gr.Slider(0.1, 10, value=1, step=0.1, label="è®­ç»ƒè½®æ•° (Epochs)", visible=False)
                        max_steps_slider = gr.Slider(10, 2000, 100, step=10, label="æœ€å¤§è®­ç»ƒæ­¥æ•° (Max Steps)", visible=True)
                        save_steps_input = gr.Number(value=50, label="æ¯ N æ­¥ä¿å­˜ä¸€æ¬¡æ–­ç‚¹", visible=True)
                        batch_size_slider = gr.Slider(1, 16, value=1, step=1, label="Batch Size")
                        grad_accum_slider = gr.Slider(1, 16, value=8, step=1, label="Gradient Accumulation")
                        optimizer_dropdown = gr.Dropdown(["adamw_8bit", "adamw_torch"], value="adamw_8bit", label="ä¼˜åŒ–å™¨")
                        learning_rate_slider = gr.Slider(1e-5, 5e-4, 2e-4, step=1e-5, label="å­¦ä¹ ç‡")
                    start_button = gr.Button("âœ… å¼€å§‹è®­ç»ƒ", variant="primary")
                # -- å³ä¾§ï¼šTensorBoard é¢æ¿ --
                with gr.Column(scale=3):
                    gr.Markdown("## ğŸ“Š TensorBoard ç›‘æ§é¢æ¿")
                    tensorboard_html = f'<iframe src="http://127.0.0.1:6006" width="100%" height="800px" frameborder="0"></iframe>'
                    tensorboard_view = gr.HTML(tensorboard_html)
            # -- åº•éƒ¨ï¼šçŠ¶æ€è¾“å‡º --
            gr.Markdown("--- \n ## ğŸ“‹ è®­ç»ƒæ—¥å¿—ä¸çŠ¶æ€")
            status_output = gr.Textbox(label="Status", interactive=False, lines=3, max_lines=5)

        # ========================== æµ‹è¯•é€‰é¡¹å¡ ==========================
        with gr.Tab("ğŸ’¬ æµ‹è¯• (Inference Playground)"):
            gr.Markdown("## ğŸ§  ä¸ä½ è®­ç»ƒçš„æ¨¡å‹å¯¹è¯")
            gr.Markdown("**æ­¥éª¤1**: é€‰æ‹©åŸºç¡€æ¨¡å‹ã€‚**æ­¥éª¤2**: é€‰æ‹© LoRA é€‚é…å™¨ã€‚**æ­¥éª¤3**: è®¾å®šç³»ç»Ÿæç¤ºã€‚")
            with gr.Row():
                inference_model_selector = gr.Dropdown(label="é€‰æ‹©åŸºç¡€æ¨¡å‹ (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´)", choices=MODEL_DISPLAY_NAMES, value=MODEL_DISPLAY_NAMES[0] if MODEL_DISPLAY_NAMES else None)
                lora_selector_dropdown = gr.Dropdown(label="é€‰æ‹© LoRA é€‚é…å™¨", choices=list_trained_loras())
            load_model_button = gr.Button("è½½å…¥æ¨¡å‹è¿›è¡Œæµ‹è¯•", variant="primary")
            load_status_textbox = gr.Textbox(label="æ¨¡å‹åŠ è½½çŠ¶æ€", interactive=False)
            
            system_prompt_textbox = gr.Textbox(label="ç³»ç»Ÿæç¤º (System Prompt)", info="ä¸ºä½ çš„ AI è®¾å®šä¸€ä¸ªèº«ä»½ã€è§„åˆ™æˆ–åŸºè°ƒã€‚", lines=3,
                value="ä½ ç°åœ¨æ˜¯é½å¤©å¤§åœ£å­™æ‚Ÿç©ºï¼Œè¯·ç”¨å­™æ‚Ÿç©ºçš„èº«ä»½å’Œé£æ ¼æ¥å›ç­”æ¥ä¸‹æ¥çš„æ‰€æœ‰é—®é¢˜ã€‚")
            
            chatbot = gr.Chatbot(label="èŠå¤©çª—å£", type="messages", height=500)
            
            with gr.Row():
                chat_input_textbox = gr.Textbox(show_label=False, placeholder="è¾“å…¥ä½ çš„æ¶ˆæ¯...", scale=4, container=False)
                submit_button = gr.Button("å‘é€", variant="primary", scale=1)
    
    # --- 5. äº‹ä»¶ç»‘å®š ---
    
    # è®­ç»ƒ Tab çš„äº‹ä»¶
    training_mode_selector.change(fn=update_training_mode_ui, inputs=training_mode_selector, outputs=[num_epochs_slider, max_steps_slider, save_steps_input])
    train_inputs = [
        experiment_name_input, resume_checkbox, training_mode_selector, num_epochs_slider, max_steps_slider, save_steps_input,
        model_dropdown, dataset_dropdown, lora_r_slider, lora_alpha_slider,
        batch_size_slider, grad_accum_slider, optimizer_dropdown, learning_rate_slider
    ]
    start_button.click(fn=train_model, inputs=train_inputs, outputs=[status_output])

    # æµ‹è¯• Tab çš„äº‹ä»¶
    load_model_button.click(
        fn=load_inference_model,
        inputs=[inference_model_selector, lora_selector_dropdown],
        outputs=[loaded_model_state, loaded_tokenizer_state, load_status_textbox]
    )
    submit_event = chat_input_textbox.submit(
        fn=run_chat,
        inputs=[chat_input_textbox, chatbot, system_prompt_textbox, loaded_model_state, loaded_tokenizer_state],
        outputs=[chatbot],
    )
    submit_event.then(lambda: gr.update(value=""), outputs=[chat_input_textbox])
    button_event = submit_button.click(
        fn=run_chat,
        inputs=[chat_input_textbox, chatbot, system_prompt_textbox, loaded_model_state, loaded_tokenizer_state],
        outputs=[chatbot],
    )
    button_event.then(lambda: gr.update(value=""), outputs=[chat_input_textbox])

    # Gradio åº”ç”¨åŠ è½½æ—¶ï¼Œè‡ªåŠ¨åœ¨åå°å¯åŠ¨ TensorBoard
    demo.load(fn=launch_tensorboard)

# --- 6. å¯åŠ¨åº”ç”¨ ---
if __name__ == "__main__":
    demo.launch(share=True)